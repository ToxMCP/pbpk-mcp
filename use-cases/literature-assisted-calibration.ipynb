{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Literature-Assisted Calibration\n",
        "\n",
        "- **Goal:** Convert extracted literature facts into parameter suggestions for manual review.\n",
        "- **Data:** First entry from `reference/goldset/index.json`.\n",
        "- **Targets:** Preserve extraction quality thresholds (`fact_accuracy \u2265 0.95`, `table_row_recall \u2265 0.9`).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Steps\n",
        "\n",
        "1. Run the extraction pipeline on a gold-set PDF.\n",
        "2. Evaluate the output against the annotated fixture.\n",
        "3. Generate parameter suggestions using the literature action mapper.\n",
        "4. Feed accepted suggestions through the analyst console or API for application.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "timeout": 0
        }
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "from mcp_bridge.literature.actions import LiteratureActionMapper\n",
        "from mcp_bridge.literature.evaluation import evaluate, load_fixture\n",
        "from mcp_bridge.literature.extractors import (\n",
        "    HeuristicTextExtractor,\n",
        "    PdfExtractKitLayoutExtractor,\n",
        "    SimpleFigureExtractor,\n",
        "    SimpleTableExtractor,\n",
        ")\n",
        "from mcp_bridge.literature.pipeline import LiteratureIngestionPipeline, PipelineDependencies\n",
        "\n",
        "goldset_root = Path(\"reference/goldset\")\n",
        "manifest = json.loads((goldset_root / \"index.json\").read_text(encoding=\"utf-8\"))\n",
        "entry = manifest[\"entries\"][0]\n",
        "pdf_path = goldset_root / entry[\"pdf\"]\n",
        "annotation_path = goldset_root / entry[\"annotation\"]\n",
        "\n",
        "pipeline = LiteratureIngestionPipeline(\n",
        "    PipelineDependencies(\n",
        "        layout_extractor=PdfExtractKitLayoutExtractor(),\n",
        "        text_extractor=HeuristicTextExtractor(),\n",
        "        table_extractor=SimpleTableExtractor(),\n",
        "        figure_extractor=SimpleFigureExtractor(),\n",
        "    )\n",
        ")\n",
        "\n",
        "extraction = pipeline.run(str(pdf_path), source_id=entry[\"sourceId\"])\n",
        "fixture = load_fixture(annotation_path)\n",
        "report = evaluate(extraction, fixture)\n",
        "print(f\"fact_accuracy={report.fact_accuracy:.3f}, table_row_recall={report.table_row_recall:.3f}\")\n",
        "\n",
        "mapper = LiteratureActionMapper(simulation_id=\"usecase-calibration\")\n",
        "suggestions = mapper.map_actions(extraction)\n",
        "print(f\"Generated {len(suggestions)} parameter suggestions\")\n",
        "for suggestion in suggestions[:5]:\n",
        "    print(f\"- {suggestion.tool_name}: {suggestion.summary}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Reference Metrics\n",
        "\n",
        "The gold-set evaluation harness enforces `fact_accuracy \u2265 0.95` and\n",
        "`table_row_recall \u2265 0.9`. If the printed values fall below those thresholds,\n",
        "investigate extraction regressions before applying calibration suggestions.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}