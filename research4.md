
Architecting the Future of Agent-Driven Science: A Framework for Secure and Interoperable Model Context Protocol Implementations


Section 1: The Model Context Protocol as a Foundational Layer for Scientific AI

The integration of artificial intelligence into scientific discovery is precipitating a fundamental transformation in research methodologies. Agentic AI systems, capable of autonomous reasoning, planning, and interaction with their environment, are moving beyond the confines of theoretical models to become active participants in the research lifecycle.1 However, the efficacy of these agents is directly proportional to the quality and accessibility of the context they are provided. The Model Context Protocol (MCP) has emerged as the foundational standard designed to bridge this gap, offering a universal, secure, and scalable framework for connecting AI agents to the vast, heterogeneous landscape of scientific data, tools, and computational resources. This section will establish the strategic importance of MCP, deconstruct its core architecture, and translate its formal specifications into practical capabilities, framing the protocol not merely as a technical standard but as a critical enabler for a new paradigm of AI-assisted discovery.

1.1. The Paradigm Shift: From Monolithic Models to Composable, Tool-Augmented Agents in Scientific Discovery

The remarkable advancements in large language models (LLMs) have been constrained by a fundamental limitation: their isolation from the dynamic, real-world systems where data is generated and actions are taken.3 For end-users, this isolation manifests as a constant and inefficient "copy and paste tango," manually shuttling information between sources and the model's interface.4 In the context of scientific research, this limitation is not just inefficient; it is a critical barrier to progress. Scientific data is not static; it resides in specialized, legacy, and often siloed systems such as Laboratory Information Management Systems (LIMS), Quality Management Systems (QMS), high-performance computing (HPC) filesystems, and real-time instrument controllers.2 The challenge of integrating AI with this diverse ecosystem is compounded by the "N×M" integration problem: for N AI models and M tools or data sources, a unique, custom-coded connector is required for each pair, resulting in a fragile and unscalable web of brittle integrations.6
Introduced by Anthropic in November 2024, the Model Context Protocol directly addresses this systemic challenge.6 Conceived as an open standard, MCP provides a universal, "plug-and-play" interface—often analogized to a "USB-C port for AI"—that replaces bespoke integrations with a single, unified protocol.4 This standardization is the critical catalyst for the development of truly agentic AI systems that can autonomously discover, plan, and execute complex, multi-step scientific workflows.1 By creating a common language for interaction, MCP enables an AI agent to seamlessly query a materials database, submit a computational job to an HPC cluster, and then analyze the results, all without requiring hardcoded logic for each specific tool.
The strategic importance of this approach was rapidly validated by the protocol's swift adoption across the AI industry. Major providers, including OpenAI and Google DeepMind, officially integrated MCP into their product ecosystems within months of its announcement, signaling a powerful industry consensus on its utility and cementing its position as the de facto standard for agent-tool communication.6 This broad support ensures that investments made in developing MCP-compliant scientific tools will have long-term viability and interoperability across a wide range of current and future AI platforms. This decoupling of the AI model from the tools it uses is a profound architectural advantage. It allows a research institution to build a robust, enduring library of scientific MCP servers for its unique instruments, databases, and computational resources, independent of any single LLM provider. As more advanced foundation models become available, this entire ecosystem of tools can be connected to the new model with minimal re-engineering, transforming the development of scientific AI from a series of model-specific, monolithic projects into the strategic curation of a universal, interoperable, and future-proof toolset.

1.2. Deconstructing the MCP Architecture: The Host-Client-Server Model

The architecture of the Model Context Protocol is intentionally straightforward, drawing inspiration from the successful design of the Language Server Protocol (LSP), which standardized the interaction between code editors and language-specific analysis tools.4 MCP employs a client-server model orchestrated by a host application, creating a clear separation of concerns that enhances security, scalability, and modularity.
The three core components of this architecture are:
MCP Host: The host is the user-facing AI application where the primary AI capabilities and orchestration logic reside. This could be a commercial application like Claude Desktop, an AI-enhanced Integrated Development Environment (IDE) such as Cursor or VS Code, or a custom-built scientific portal or "AI co-pilot".7 The host process is not the AI model itself but rather the platform that manages user interaction, gathers and prepares context, and serves as the primary security layer, orchestrating the connections between multiple AI clients and their corresponding servers.9
MCP Client: Residing within the host, an MCP client is a dedicated connector that manages a stateful, one-to-one relationship with a specific MCP server.7 Its primary responsibilities include initiating the connection, performing a "handshake" to negotiate capabilities with the server, formatting requests according to the MCP standard, processing responses, and enforcing security boundaries to ensure that data and access for one AI task or user session remain isolated from others.9
MCP Server: An MCP server is a lightweight, standalone process that acts as a smart adapter, exposing a specific tool, data source, or API to the broader MCP ecosystem.7 For example, a GitHub MCP server translates a standardized request like "list my open pull requests" into a native GitHub API call.15 The server is responsible for implementing the MCP standard for its particular system, handling authentication, and translating the results from the underlying system back into the standardized MCP response format that the client can understand.9
Communication between these components is governed by a well-defined protocol stack. At the message layer, MCP mandates the use of JSON-RPC 2.0 for all communication.4 This lightweight, text-based Remote Procedure Call protocol provides a structured and consistent format for all requests, successful responses (result), error responses (error), and one-way notifications, ensuring interoperability between clients and servers regardless of their implementation language.11
This message layer is transported over one of two primary mechanisms specified by the protocol:
Standard Input/Output (stdio): This method is ideal for local MCP servers that run in the same environment as the host application. Communication occurs directly through the standard input and output streams of the server process, offering a fast, low-latency, and secure channel for local interactions.4
HTTP with Server-Sent Events (SSE): This transport is preferred for connecting to remote servers over a network. The client sends requests via standard HTTP POST, and the server can stream responses and notifications back to the client using the SSE protocol. This leverages widely adopted web standards for efficient, real-time data streaming.4
This clean architectural separation has critical implications for security and trust. The MCP specification itself defines security principles, such as the need for user consent before accessing data or executing tools, but it explicitly states that it cannot enforce these principles at the protocol level.14 This responsibility falls squarely on the implementors, and most critically, on the MCP Host. The host application is the final gatekeeper, responsible for rendering the consent user interface, managing the authorization flows, and potentially sandboxing server interactions to protect the user's system.9 Recent security research has demonstrated that users struggle to identify malicious servers from public registries alone, making the host's role as the user's trusted interface paramount.22 Therefore, any robust strategy for building a secure scientific MCP ecosystem must focus heavily on the architecture, UI, and security features of the host application, which ultimately mediates the agent's power and the user's trust.

1.3. The MCP Specification in Practice: Translating Protocol Features into Scientific Capabilities

The formal MCP specification defines a rich set of features that, when implemented, provide the building blocks for creating sophisticated, tool-augmented AI agents. The interaction between a client and server begins with a critical capability negotiation phase, often referred to as the "handshake".9 During the initial initialize request, the client and server exchange information about the features they support.19 This process forms a contract that governs their subsequent interaction, allowing them to use only mutually supported features, which prevents runtime errors and enables graceful degradation of functionality if a feature is unavailable.9
Once the connection is established, an MCP server can expose its functionality through three core primitives, each with direct applicability to scientific workflows 9:
Resources: These represent read-only contextual data that a server can provide to the AI model or the user. Resources are the mechanism for grounding an agent in real-time, domain-specific information, helping to reduce hallucinations and improve the relevance of its responses.7
Scientific Example: An MCP server for the Protein Data Bank (PDB) could expose each protein structure as a Resource. An agent could then request the resource for PDB ID 6M0J (the SARS-CoV-2 spike protein) to retrieve its structural data, publication information, and associated metadata.
Tools: These are callable functions that have side effects, enabling an AI agent to perform actions in the real world.9 Tools are the primary mechanism for an agent to move from being a passive information processor to an active participant in the research process.
Scientific Example: An MCP server for a high-performance computing (HPC) cluster's Slurm scheduler could expose a submit_job tool. This tool would take parameters such as the job script path, partition, and resource allocation. An agent could then use this tool to programmatically launch a molecular dynamics simulation, effectively acting as an automated research assistant.
Prompts: These are server-provided templates for common, multi-step tasks that involve a curated sequence of resource access and tool calls.9 Prompts act as shortcuts or pre-defined workflows that guide the agent, simplifying complex operations for the user.
Scientific Example: A server for a bioinformatics pipeline might offer a run_phylogenetic_analysis prompt. When invoked, this prompt could guide the agent through the necessary sequence of tool calls: using a fetch_sequences tool to get data from NCBI, an align_sequences tool to run MAFFT, and finally a build_tree tool to run RAxML, encapsulating an entire standard operating procedure into a single, high-level command.
In addition to these server-exposed features, the protocol defines advanced capabilities that a client can offer to a server, enabling more dynamic and collaborative interactions. The most powerful of these is Sampling.11 This feature allows a server to initiate an agentic behavior in the client by sending it a request to process a prompt and return the result. This inverts the typical flow of control and is essential for building truly interactive tools where the tool itself may need to ask the AI for assistance, such as summarizing a large data file or making a decision based on intermediate results. While powerful, this feature also carries significant security risks, as it could be used by a malicious server to hijack the agent's reasoning loop, a threat known as a "Puppet Attack".22 Consequently, its use must be governed by strict user consent and host-side controls.

Section 2: A Comprehensive Security Framework for Scientific MCP Implementations

The integration of autonomous, non-deterministic AI agents into scientific and regulated environments represents a profound shift in the enterprise risk surface.5 While MCP provides a standardized protocol for connectivity, it is not a security panacea. Connecting agents to sensitive systems—such as quality management systems (QMS) in biopharma, laboratory information management systems (LIMS) containing raw experimental data, or electronic health records (EHRs) with patient information—introduces a new class of vulnerabilities that demand a multi-layered, defense-in-depth security framework. This section will establish a comprehensive security model tailored to the high-stakes context of scientific research, synthesizing official protocol best practices with findings from the latest security analyses of the MCP ecosystem.

2.1. Threat Modeling the Scientific Agent: A High-Stakes Environment

A robust security posture begins with a clear understanding of the threats. The expanded attack surface created by MCP-enabled agents encompasses both classic vulnerabilities re-contextualized for an agentic world and a new class of threats specific to the protocol's architecture.
Classic Vulnerabilities in an Agentic Context:
Prompt Injection: This is perhaps the most well-known vulnerability, where an adversary crafts malicious input designed to manipulate the agent into executing unintended actions.5 In a scientific context, this could involve an agent analyzing a public dataset that contains a hidden prompt, tricking it into using a connected ssh_exec tool to exfiltrate proprietary, unpublished research data from a local server. The boundary between instruction and data is blurred, making any untrusted input a potential attack vector.5
Function Over-Permissioning and the Confused Deputy Problem: This threat arises when an agent is granted broad permissions that exceed the authority of the user on whose behalf it is acting.5 The agent, acting as a "confused deputy," can be tricked by a low-privilege user's input into calling a high-privilege tool. For example, an agent accessible to a junior lab technician might be granted permissions to recalibrate sensitive instruments. A cleverly worded prompt could trick the agent into invoking this recalibrate_instrument tool, potentially invalidating months of experimental data.
Data Exfiltration: Unlike traditional data loss prevention (DLP) which can often be managed with simple pattern matching, LLM agents can summarize, translate, or otherwise transform sensitive data before returning it, effectively bypassing standard filters.5 An agent asked to summarize clinical trial notes for a report could inadvertently include and obfuscate Protected Health Information (PHI), leading to severe compliance violations.
Emerging, MCP-Specific Attack Vectors:
Recent empirical studies of the MCP ecosystem have identified sophisticated attacks that exploit the protocol's architecture and the trust relationship between users, hosts, and servers 22:
Tool Poisoning: A malicious MCP server advertises a benign-sounding tool (e.g., visualize_data), but the underlying implementation contains harmful code that executes when the tool is called. This could range from scanning the local network to installing malware.
Puppet Attacks: A malicious server exploits advanced protocol features like sampling to hijack the agent's reasoning process. The server can send a request to the agent's LLM, effectively turning the agent into a "puppet" to perform actions of the server's choosing, potentially without the user's knowledge.
Rug Pull Attacks: This is a social engineering attack on trust. A server initially provides a genuinely useful and benign tool. After gaining the trust of users and being widely adopted, the developer updates the server to include malicious functionality, exploiting the established trust to bypass scrutiny.
The only viable security model for navigating this complex threat landscape is a Zero Trust architecture. The protocol's native model of negotiated trust between client and server is insufficient. A robust scientific implementation cannot implicitly trust the agent, the server, or the network. Every interaction must be independently authenticated, authorized against fine-grained policies, and monitored for anomalous behavior. This shifts the security paradigm from securing the perimeter of the endpoints to securing the interactions themselves, a critical mindset for building a resilient scientific agent ecosystem.
The following table provides a clear, actionable summary of these key security threats, translating their impact into a specific scientific context and mapping them to concrete mitigation strategies. It serves as a vital risk assessment tool for architects and security officers.
Table 1: MCP Security Threat Matrix for Scientific Research
Threat
Description
Scientific Research Impact Example
Mitigation Strategy
Prompt Injection
Malicious input tricks an LLM into performing unintended actions.
An agent analyzing a public dataset is prompted to use an SSH tool to exfiltrate unpublished research data from a connected server.
Input Sanitization, Schema Enforcement, Strict RBAC (Tool Scoping), Human-in-the-Loop for sensitive operations.
Confused Deputy
An agent with broad permissions is tricked by a low-privilege user into performing a high-privilege action.
A junior researcher's agent is tricked into calling a tool that modifies a validated instrument calibration profile, invalidating subsequent experiments.
Principle of Least Privilege, Fine-Grained Tool-Level Permissions, Short-Lived, Scoped Credentials.
Data Exfiltration
An agent summarizes or transforms sensitive data to bypass simple output filters.
An agent asked to summarize clinical trial notes for a report inadvertently includes and obfuscates Patient Health Information (PHI) in its output.
Runtime Monitoring, Context-Aware Data Loss Prevention (DLP), Output Sanitization, Immutable Audit Logging.
Tool Poisoning
A malicious server offers a tool that performs harmful actions under the guise of a benign function.
A community-provided "Data Visualization" server secretly contains code that scans the local network for open ports or mines cryptocurrency.
Server Sandboxing (Containerization), Code Signing, Trusted Server Registries, Runtime Behavior Monitoring.
Puppet Attack
A malicious server uses advanced features like sampling to hijack the agent's reasoning process.
A server for a chemical database, when queried, uses a sampling request to force the agent to perform a series of unauthorized trades on a connected financial API.
Strict Host-side control over sampling, User Consent for all server-initiated actions, Disabling high-risk features from untrusted servers.


2.2. The Authentication and Authorization Backbone: Identity and Control

A secure MCP implementation must be built on a foundation of strong, verifiable identity. This requires moving beyond simplistic authentication methods and embracing enterprise-grade standards for both authentication and authorization.
Authentication: The use of static, long-lived tokens or API keys in production environments is a significant security risk; they are difficult to rotate, audit, and are often accidentally committed to version control.24 The unequivocal best practice is to mandate the use of modern, token-based authentication standards. OAuth 2.0 / OpenID Connect (OIDC) should be the default mechanism for all MCP server access.5 This allows the system to leverage existing enterprise identity providers (e.g., a university's Single Sign-On system), providing centralized control and a seamless user experience. Furthermore, security measures such as enforcing short-lived access tokens and using the Proof Key for Code Exchange (PKCE) extension can significantly harden the authentication flow against interception attacks.24 This is particularly critical in federated research environments that span multiple institutions, a challenge highlighted in practical HPC integration studies.13
Authorization (RBAC): Authentication confirms who a user is; authorization determines what they are allowed to do. In an MCP ecosystem, permissions must not be granted at the server level but at the individual tool level.24 Implementing a fine-grained Role-Based Access Control (RBAC) system is non-negotiable. This means defining roles (e.g., Lab_Technician, Principal_Investigator, Instrument_Admin) and assigning specific tool permissions to each role. For example, a Lab_Technician might be granted access to a read_instrument_data tool but be denied access to a recalibrate_instrument tool. This enforces the principle of least privilege, ensuring that any user or agent can only access the absolute minimum set of capabilities required for their function, dramatically reducing the potential impact of a compromised account or a confused deputy attack.24
Consent and Authorization Flows: A critical security principle outlined in the MCP specification is that of explicit user consent.14 The implementation of this flow is paramount. The MCP server must not be allowed to directly pass through third-party tokens, as this can circumvent security controls.26 Instead, the MCP Host must mediate a secure consent workflow. When a server requires access to a third-party service (e.g., Google Drive), the host must redirect the user to the third-party's official authorization server.27 The consent screen must clearly identify the requesting client and the specific API scopes being requested. To prevent Cross-Site Request Forgery (CSRF) attacks, the flow must use a cryptographically secure, single-use state parameter.26 Once consent is granted, the decision should be stored securely, for example, in a cryptographically signed, HttpOnly cookie bound to the specific client ID.26

2.3. Runtime Defense and Observability: Trust but Verify

Static security controls like authentication and authorization are necessary but insufficient. The dynamic and non-deterministic nature of AI agents requires a robust layer of runtime defense and deep observability to monitor, detect, and respond to threats as they occur.
Input Validation and Schema Enforcement: Every request arriving at an MCP server must be treated as potentially hostile. The first line of defense is rigorous input validation. All incoming JSON-RPC requests must be validated against a predefined schema that enforces strict constraints on data types, lengths, and ranges.24 Any request that is malformed, contains unrecognized parameters, or violates the schema must be rejected immediately. This practice is a primary defense against a wide range of injection attacks.25 All inputs should be normalized and properly escaped before being passed to downstream systems, especially those that perform database queries or file system operations.24
Runtime Threat Detection and Data Loss Prevention (DLP): A sophisticated security posture requires monitoring the live interactions between agents and tools. This can be achieved by routing all MCP traffic through a central gateway or proxy that provides runtime security services.24 Such a system can perform deep packet inspection to detect anomalous behavior indicative of a prompt injection attack or a malicious tool execution. Furthermore, it can apply dynamic, context-aware guardrails. For example, a policy could automatically block a tool from returning data that matches a pattern for Personally Identifiable Information (PII) or enforce a rule that an agent may not access financial APIs outside of business hours.27 This inline policy enforcement provides a critical layer of defense against unintentional data leakage and malicious activity.24
Immutable Audit Trails: In scientific research, and particularly in regulated fields like medicine and pharmaceuticals, data integrity, traceability, and reproducibility are paramount. Compliance with standards such as GxP and 21 CFR Part 11 requires that every action that creates, modifies, or deletes regulated data is logged with clear attribution.5 The base MCP protocol does not have a built-in, standardized audit trail mechanism, representing a critical gap for scientific applications.5 Therefore, it is essential to implement a comprehensive and immutable logging system at the infrastructure level. Every agent action—every tool call, every parameter passed, every piece of data accessed, and every result returned—must be logged in a centralized, tamper-evident audit trail. This log serves not only as a crucial tool for security forensics but also as the digital equivalent of a lab notebook, providing the traceability required to validate and reproduce AI-driven scientific results.

2.4. Secure Deployment and Governance: Building a Fortress

The security of an MCP ecosystem depends not only on the protocol and the code but also on the operational environment in which it is deployed and the governance framework that oversees it.
Sandboxing and Isolation: A fundamental principle of secure deployment is to run each MCP server in an isolated, sandboxed environment.29 The most effective and widely recommended approach is to package each server as a Docker container.30 This provides a standardized, portable, and isolated environment that includes all necessary dependencies. This container should be run with the minimum necessary privileges, and its access to the host's file system and network should be strictly limited by the container runtime's security policies. For example, a server that only needs to interact with a local database should be prevented from making any outbound network connections. This containment strategy is a primary defense against tool poisoning attacks, as it limits the potential damage a malicious server can inflict.24
Secure Coding and Dependency Management: The development process for MCP servers must adhere to secure coding best practices. This includes fundamental principles like using parameterized queries to prevent SQL injection and treating all stored content retrieved from databases or files as untrusted until validated.27 A significant vector for vulnerabilities is the use of insecure third-party libraries and dependencies. It is essential to integrate automated security scanning tools, such as Snyk, into the continuous integration pipeline.30 These tools scan the project's dependencies against a comprehensive database of known vulnerabilities, alerting developers to risks and ensuring that the software supply chain is secure.
Human-in-the-Loop (HITL) for High-Risk Operations: For any action that is irreversible or carries significant risk—such as modifying a production database, deleting experimental data, or executing arbitrary code on a remote machine via SSH—the system must enforce a mandatory human approval step.25 This is not merely a suggestion but a critical control. The MCP Host application's user interface must present the proposed action to the user in a clear, unambiguous, and non-truncatable format, detailing the exact command to be executed and all of its parameters.26 The user must then provide explicit approval before the action can proceed. This HITL workflow acts as a final, common-sense backstop against both malicious attacks and unintentional but catastrophic agent errors.
The tension between the goal of broad tool discoverability and the security principle of least privilege is a central architectural challenge in designing a secure MCP ecosystem. The power of agentic AI is unlocked when an agent can dynamically discover and compose a vast array of available tools to solve a novel problem.15 However, from a security perspective, exposing an agent to a large number of tools dramatically expands the attack surface for prompt injection and confused deputy attacks.24 An agent that can see every tool is an agent that can be tricked into misusing any of them. This conflict cannot be resolved by the protocol alone; it requires an architectural solution. The most effective approach is to implement a dynamic, context-aware authorization layer. Instead of granting a worker agent a static set of permissions, a higher-level "manager" agent or an orchestration framework (like LangChain 10) should, for a given task, provision a temporary, task-specific subset of authorized tools. This provides the agent with the precise capabilities it needs for the immediate context while minimizing the available attack surface at any given moment, effectively balancing power with safety.

Section 3: Designing Universal and Robust Scientific Tools

Building a successful MCP ecosystem for science requires more than just secure connections; it demands the creation of tools that are discoverable, comprehensible, reliable, and performant. The design of the tool's interface—its schema, its communication patterns, and its resilience mechanisms—is as critical as the protocol that carries its messages. This section provides an architectural blueprint for designing scientific tools that are not only connectable via MCP but are also truly universal and robust enough to support the rigors of scientific workflows.

3.1. The Blueprint for Interoperability: Leveraging the OpenAPI Specification (OAS)

While MCP provides the standardized transport protocol for agent-tool communication, it does not, by itself, provide a rich, machine-readable description of a tool's capabilities. To achieve true "plug-and-play" interoperability, a complementary standard is needed to describe the semantics of the tool's API. The OpenAPI Specification (OAS), formerly known as Swagger, has emerged as the industry standard for this purpose.31
OAS provides a language-agnostic, structured format for describing RESTful APIs, detailing the available endpoints, the required parameters, authentication methods, and the structure of expected responses.31 For an AI agent, an OpenAPI document is more than just documentation; it is a machine-readable manual that allows it to understand and interact with an API without any prior, hardcoded knowledge.31 The synergy between MCP and OAS is powerful: MCP provides the pipe, and OAS describes what can be sent through it.
The primary benefit of this approach is the potential for automated tool generation. Leading AI agent frameworks, including LangChain, Haystack, and Google's Agent Development Kit (ADK), are designed to ingest an OpenAPI document and automatically generate a complete set of callable agent tools from it.33 This dramatically accelerates development, reduces the chance of human error, and ensures that the agent's understanding of the API is always perfectly synchronized with its specification.31 For a research institution, this means that any existing scientific service with a well-defined OpenAPI specification can be made available to AI agents with minimal additional effort.
To maximize the effectiveness of this approach, OpenAPI specifications should be written with an AI agent as the intended consumer. This involves adhering to several best practices:
Use Clear and Descriptive operationIds: The operationId for each API endpoint is often used to generate the tool's name. It should be a clear, verb-noun combination (e.g., get_material_by_id, submit_hpc_job) that is easily understood by an LLM.34
Write Detailed Natural Language Descriptions: The summary and description fields for each operation and parameter are critical. They are used to populate the tool's description, which the LLM uses to decide when and how to call the tool. These fields should be written in clear, natural language, explaining the tool's purpose, its parameters, and what it returns, as if explaining it to a human assistant.34
Provide Concrete Examples: Including examples of valid requests and responses within the specification can significantly help the LLM understand the expected data formats and construct valid API calls.
This combination of MCP and OAS creates a self-documenting and self-integrating ecosystem for scientific infrastructure. The traditional process of integrating a new instrument or database, which often requires months of developer effort to read documentation and write custom client code, can be transformed into an automated, agent-driven task. A central service registry can ingest the OpenAPI specifications of all available tools, making them discoverable. An AI agent can then parse the specification to learn how to use a new tool on the fly, dramatically lowering the barrier to entry for using complex scientific resources and accelerating the pace of research.

3.2. Communication Protocol Deep Dive: JSON-RPC vs. gRPC

The MCP specification standardizes on JSON-RPC 2.0 as its base communication protocol, typically transported over stdio for local connections or HTTP for remote ones.17 This choice offers several advantages: it is built on ubiquitous web standards, it is human-readable which aids in debugging, and its request-response model is well-suited for the command-and-control interactions typical of tool use. For the majority of scientific use cases—such as querying a database, submitting a job, or retrieving metadata—JSON-RPC provides a perfectly adequate and robust solution.
However, certain scientific applications have performance and data-streaming requirements that can push the limits of a text-based, request-response protocol. Use cases such as streaming high-frequency data from a particle detector, real-time telemetry from a bioreactor, or large-scale data transfers between HPC facilities demand a more performant solution. For these scenarios, gRPC, a high-performance RPC framework developed by Google, presents a compelling alternative.35
gRPC's key advantages stem from its use of HTTP/2 as its transport layer and Protocol Buffers (Protobuf) as its data serialization format.35 This provides:
High Performance: Protobuf serialization is a binary format that is significantly more compact and faster to parse than JSON, reducing latency and network bandwidth usage.36
Full-Duplex Streaming: Unlike the request-response model of HTTP/1.1, HTTP/2 and gRPC natively support bidirectional, full-duplex streaming, allowing both the client and server to send a continuous stream of messages to each other over a single, long-lived connection.35 This is ideal for real-time data monitoring and interactive applications.
Given these trade-offs, the optimal architectural recommendation for a comprehensive scientific MCP ecosystem is a hybrid approach. The standard MCP protocol using JSON-RPC should be used for the primary control plane: tool discovery, capability negotiation, metadata operations, and executing standard command-like tools. For use cases that require high-bandwidth data streaming, an MCP tool should not attempt to send the data through the primary JSON-RPC channel. Instead, it should provide a method to negotiate and establish a separate, dedicated gRPC connection for the data plane. This pattern allows the ecosystem to benefit from the standardization and web-friendliness of MCP for general-purpose interactions, while leveraging the raw performance of gRPC where it is most needed. A concrete example of this hybrid philosophy can be seen in the Agent Interaction Protocol (AIP) developed by the Chinese Academy of Sciences, which uses gRPC as its native transport but defines an "MCP proxy node" to allow for seamless integration and unified management of standard MCP services within its gRPC-based system.36

3.3. Building for Resilience: Ensuring Robust Scientific Workflows

Scientific workflows are often complex, long-running, and prone to transient failures. Tools designed for this environment must be built with resilience as a core principle. This involves robust error handling, mechanisms for managing long-running tasks, and adherence to practical conventions that improve debuggability.
Error Handling and Graceful Degradation: An agent's ability to reason and self-correct depends on receiving clear, structured feedback when something goes wrong. MCP servers must implement robust error handling as defined in the JSON-RPC 2.0 specification.11 Instead of returning a generic failure message, the server should provide a structured error object containing a specific error code and a descriptive message. This allows a sophisticated agent to potentially diagnose the problem (e.g., "invalid_parameter," "authentication_failed") and retry the operation with corrected parameters or prompt the user for assistance.
Managing Long-Running Tasks: Many scientific computations, such as simulations or large-scale data analyses, can take hours, days, or even weeks to complete. A simple request-response model is insufficient for managing these tasks. The MCP specification provides two essential features for this purpose: progress notifications and cancellation support.14 The server should periodically send progress notifications to the client, providing status updates on the task's execution. This allows the host application to keep the user informed and helps prevent timeouts. Equally important, the server must be designed to honor cancellation requests from the client. If a user decides to abort a long-running job, the server should receive the cancellation request and gracefully terminate the underlying process, avoiding the waste of valuable computational resources.
Practical Implementation Conventions: Beyond the formal specification, practical experience has revealed several conventions that improve the reliability and usability of MCP servers.
Tool Naming: LLMs are sensitive to the way text is tokenized. Experience has shown that using snake_case (e.g., get_npm_package_info) for tool names leads to more reliable parsing and invocation by models like GPT-4o, whereas names containing spaces or dots can cause confusion.30
Structured Logging: For servers using the stdio transport, standard console output is reserved for JSON-RPC messages. Therefore, it is essential to implement structured file logging using a dedicated library (e.g., pino for Node.js) to record diagnostic information and program flow, which is invaluable for debugging.30
The design of a tool's schema, whether in OpenAPI or as a native MCP tool definition, has a direct and profound influence on the agent's emergent reasoning capabilities. An agent does not simply execute a tool; it reasons about which tool to call and how to call it based on the natural language descriptions provided in the schema.33 A vaguely described or poorly named tool will inevitably lead to the agent making incorrect choices or "hallucinating" parameters. Conversely, a well-designed schema can act as a form of scaffolding, guiding the agent's planning process. For instance, by breaking a complex scientific analysis into a series of smaller, composable tools and using their description fields to articulate their inputs and outputs (e.g., "This tool refines a crystal structure. It requires the output file from the run_dft_calculation tool."), the schema itself becomes a form of indirect prompt engineering. It provides the agent with the necessary clues to construct a valid and logical workflow, significantly improving the reliability and predictability of its autonomous behavior. Therefore, schema design should be treated not as mere documentation, but as a critical component of the agent's cognitive architecture.

Section 4: Practical Implementation and Case Studies in Scientific Domains

The architectural and security principles outlined in the preceding sections are not merely theoretical constructs. They are being actively implemented and tested in real-world scientific applications. This section grounds the report in these practical implementations, showcasing how MCP is being used to build open-source tools for research, bridge the gap to existing research cyberinfrastructure, and enable complex multi-agent collaborations. These case studies provide both a practical roadmap for adoption and valuable lessons learned from early pioneers.

4.1. The mcp.science Ecosystem: Open-Source Tools for Research

A leading example of MCP's application in the scientific domain is the mcp.science project, an open-source initiative by the Path Integral Institute.10 This project provides a curated collection of MCP servers specifically designed to connect AI agents with common scientific data sources, tools, and computational resources, serving as a powerful demonstration of the protocol's potential.37
A deep dive into the key servers within the mcp.science collection reveals the breadth of capabilities that can be exposed to an AI agent:
Materials Project: This server provides a direct interface to the Materials Project database, a critical resource for computational materials science. It allows an agent to programmatically search, visualize, and manipulate materials science data, enabling complex queries such as "Find all perovskite structures with a band gap between 1.5 and 2.0 eV".37
Python Code Execution: A crucial tool for analysis and computation, this server allows an agent to execute Python code snippets in a secure, sandboxed environment. This enables an agent to perform on-the-fly statistical analysis, data transformation, or plotting of results retrieved by other tools.37 The security of the sandboxing mechanism is of paramount importance here.
SSH Exec: This server grants an agent the ability to run commands on remote machines over SSH. While incredibly powerful for interacting with HPC clusters or lab servers (e.g., to check the status of a simulation with squeue), it also represents a significant security risk. Its use must be governed by a strict, centrally managed whitelist of pre-validated, allowed commands to prevent arbitrary remote code execution.37
Jupyter-Act: Recognizing the central role of Jupyter notebooks in modern data science, this server allows an agent to interact directly with a running Jupyter kernel. It can execute cells, retrieve outputs, and effectively drive an interactive data analysis workflow within an existing notebook, blending agentic automation with a familiar research environment.37
GPAW Computation: This server exposes a highly specialized scientific tool, providing access to density-functional-theory (DFT) calculations via the GPAW package. It allows an agent to programmatically perform complex quantum mechanical calculations, such as determining the electronic structure of a novel molecule.39
To support this ecosystem, the Path Integral Institute also provides the mcpm command-line tool, an MCP package manager that simplifies the discovery, installation, and configuration of these and other community-provided servers.39 This supporting infrastructure is vital for lowering the barrier to entry and encouraging wider adoption among researchers.
The following table provides an at-a-glance reference for the capabilities of key mcp.science servers, showcasing the concrete value of MCP by connecting the protocol to real tools that can solve tangible research problems.
Table 2: mcp.science Server Capabilities and Use Cases
Server Name
Core Functionality
Example Scientific Use Case
Key Security Consideration
Materials Project
Search, visualize, and manipulate materials data from the Materials Project database.
An agent searches for all perovskite structures with a band gap between 1.5-2.0 eV for solar cell research.
Requires secure storage and handling of a Materials Project API key.
Python Code Execution
Executes Python code snippets in a secure, sandboxed environment.
An agent uses this tool to perform statistical analysis or plot results from data retrieved by another tool.
The sandboxing environment must be robustly configured to prevent escape and access to the host system.
SSH Exec
Runs pre-validated commands on remote machines over SSH.
An agent logs into an HPC cluster to check the status of a running simulation using squeue.
CRITICAL: Must use a strict, centrally managed whitelist of allowed commands to prevent arbitrary code execution.
Jupyter-Act
Interacts with a running Jupyter kernel to execute cells and retrieve outputs.
An agent drives an interactive data cleaning and visualization workflow within an existing Jupyter notebook.
The Jupyter kernel has access to the user's environment; access to this tool should be tightly controlled.
GPAW Computation
Provides access to density-functional-theory (DFT) calculations via the GPAW package.
An agent programmatically calculates the electronic structure of a newly proposed molecule.
Computational tools can be resource-intensive; requires rate-limiting and cost-tracking mechanisms.


4.2. Bridging to Research Cyberinfrastructure (CI): Lessons from HPC

While projects like mcp.science demonstrate the creation of new, agent-native tools, a significant challenge for research institutions is integrating AI agents with decades of investment in existing, mature, and often heterogeneous research cyberinfrastructure (CI).13 A recent paper from researchers working with high-performance computing facilities provides a crucial, pragmatic blueprint for tackling this challenge.13
The core lesson from this work is that the most successful adoption strategy for MCP in established scientific environments is not to "rip and replace" existing services, but to "wrap and extend" them. The researchers implemented thin MCP servers that act as lightweight adapters over mature, trusted services like Globus (for data transfer, remote computation, and search) and facility-specific status APIs. This approach provides a modern, unified, agent-accessible interface on top of the existing, reliable infrastructure, dramatically lowering the political and technical barriers to adoption.
Their case studies demonstrate the power of this architecture in practice:
In a multi-site bioinformatics pipeline, an agent used MCP servers for Globus to manage authentication, job submission, and data transfer between two different supercomputing centers (ALCF Polaris and NERSC Perlmutter), seamlessly orchestrating a complex workflow that spanned administrative domains.13
The work highlighted the remarkable resilience of agent-driven workflows. Equipped with these MCP tools, agents demonstrated an ability to recover from transient failures, adapt to changing resource availability, and dynamically adjust their strategy when an initial approach failed, adding a layer of flexibility that is often absent in rigid, hardcoded scientific scripts.13
This "thin adapter" strategy is arguably the most important strategic takeaway for any research institution looking to leverage agentic AI. The path to an AI-enabled research environment is not to rebuild from scratch, but to conduct a systematic inventory of existing, high-value services and prioritize the development of thin MCP wrappers for them.

4.3. Multi-Agent Collaboration: Beyond a Single Agent and its Tools

As scientific problems become more complex, they often require the collaboration of multiple specialized experts. The same is true for AI agents. While MCP excels at standardizing agent-to-tool communication, many advanced scientific workflows will require collaboration between multiple, specialized agents—for example, a "Librarian" agent that performs literature searches, an "Experimenter" agent that runs simulations, and a "Theorist" agent that analyzes the results and formulates new hypotheses.
This necessitates a second type of protocol: one for agent-to-agent communication. IBM's Agent Communication Protocol (ACP) is a leading open standard designed for this purpose.43 The key architectural insight is that a mature scientific agent ecosystem will likely be a two-protocol system: MCP connects agents to their tools and knowledge, while ACP connects agents to each other.44 ACP is built on a RESTful, peer-to-peer architecture that allows agents to discover each other, delegate tasks, and collaborate without needing a central orchestrator, moving beyond a simple "manager-worker" pattern to enable more sophisticated, decentralized multi-agent systems.44
This distinction is not merely a product differentiator; it is a fundamental architectural principle. A system relying solely on MCP for all communication will eventually hit a ceiling of complexity that can only be overcome by a true multi-agent system. Therefore, a forward-looking institutional strategy must plan for the adoption and integration of both types of protocols to enable the kind of collaborative, autonomous science that will drive future discoveries.
The following table provides critical context by positioning MCP within the broader landscape of agent protocols, helping to clarify its specific role and the need for a multi-protocol strategy in building a complete agentic system.
Table 3: Comparative Analysis of AI Agent Protocols
Protocol
Primary Use Case
Communication Paradigm
Transport Layer
Key Features
MCP (Model Context Protocol)
Agent-to-Tool
Client-Server RPC
JSON-RPC 2.0 (over stdio, HTTP+SSE)
Tool/Resource discovery, capability negotiation, stateful connections.
ACP (Agent Communication Protocol)
Agent-to-Agent
RESTful, Peer-to-Peer
HTTP
Agent discovery, task delegation, synchronous & asynchronous interaction.
AIP (Agent Interaction Protocol)
Agent-to-Agent & Agent-to-Tool
Client-Server & Gateway RPC
gRPC
High-performance binary transport, gateway routing for agent groups, MCP compatibility via proxy.
AG-UI (Agent-User Interaction)
Agent-to-Frontend
Event-Driven
WebSockets, SSE, Webhooks
Real-time human-in-the-loop interaction, streaming state updates.


Section 5: Future Directions and Strategic Recommendations

The Model Context Protocol provides a robust technical foundation for the future of agent-driven science. However, realizing this future requires more than just technical implementation. It demands a strategic approach to navigating the challenges of a nascent ecosystem, a forward-looking perspective on the evolution of autonomous research, and a clear, phased blueprint for adoption within research institutions. This final section synthesizes the report's findings into a set of actionable recommendations designed to guide strategic leaders in building a secure, scalable, and impactful scientific agent ecosystem.

5.1. Addressing Ecosystem Challenges: Navigating the "Wild West"

The MCP ecosystem, while growing rapidly, is still in its early stages and faces several significant challenges that must be addressed for it to mature into a trusted platform for science.
The Quality and Trust Problem: The proliferation of public MCP server registries has created a "Wild West" environment. A large-scale empirical study of the ecosystem, using a framework called "MCPCrawler," found that more than half of the 17,630 aggregated entries were invalid, placeholders, or low-value prototypes.45 This noise makes it difficult for users to find high-quality, reliable tools. More alarmingly, other research has shown that users consistently struggle to identify malicious servers on aggregator platforms, often installing them unknowingly.22 For science, where data integrity and trust are non-negotiable, relying on unvetted public registries is untenable. This highlights the urgent need for the creation of curated, trusted registries specifically for scientific tools, where servers are vetted for quality, security, and adherence to best practices.
Protocol Fragmentation and Standardization: While MCP provides a strong standard for the transport layer, there is a risk of fragmentation at the semantic layer. If every institution defines its own unique schema for common tools (e.g., a "submit_job" tool), the promise of universal interoperability is undermined. To combat this, the scientific community should champion and contribute to community-driven efforts to standardize tool schemas for common scientific domains, potentially leveraging frameworks like the Open Agentic Schema Framework (OASF).46
Security as a Shared Responsibility: Security in the MCP ecosystem is not solely the responsibility of the server developer. As demonstrated throughout this report, it requires a multi-pronged, collaborative approach. Protocol governance bodies must continue to evolve the specification with security in mind; infrastructure providers must build secure, Zero Trust gateways; host application developers must implement robust consent and sandboxing mechanisms; and institutions must invest in user education to raise awareness of the risks, such as prompt injection and malicious tool installation.22

5.2. The Path to Autonomous Science: A Forward-Looking Perspective

Looking beyond the immediate challenges, the long-term vision for MCP-enabled science is transformative. The goal is to move beyond simple task automation towards a future where AI agents can participate in the full, cyclical process of scientific discovery.1
In this future, an agent, empowered by a mature and secure ecosystem of MCP tools, could begin by conducting a comprehensive literature review using tools connected to arXiv, PubMed, and other scholarly databases.23 Based on this review, it could identify gaps in existing knowledge and formulate novel, testable hypotheses.2 It could then design an experiment, using tools that interface with simulation software or even robotic lab equipment, and execute that experiment via MCP servers connected to HPC clusters or instrument controllers. Upon completion, the agent would use analysis tools to process the results, compare them against its initial hypothesis, and generate visualizations and reports. Finally, it could propose new avenues of inquiry based on its findings, thus closing the loop and initiating the next cycle of discovery.
This vision will likely be realized not by a single monolithic agent, but by composable, collaborative multi-agent systems. These "digital research teams," composed of specialized agents communicating via protocols like MCP and ACP, will work alongside human scientists, augmenting their capabilities and accelerating the pace of discovery in complex, multi-disciplinary fields such as personalized medicine, materials discovery, and climate modeling.

5.3. Strategic Blueprint for Adoption: Actionable Recommendations

For a research institution, moving from the current state to this future vision requires a deliberate, phased approach. The following strategic blueprint provides a roadmap for adopting and scaling the use of MCP-enabled AI agents in a secure and sustainable manner.
Phase 1: Pilot and Educate (Months 1-6). The journey should begin with a small, controlled, and high-impact pilot project. Identify a single, well-understood internal service—such as a lab-specific database or a commonly used analysis script—and develop a "thin adapter" MCP server for it. This initial project serves two purposes: it builds essential in-house expertise in MCP development and security best practices, and it provides a tangible demonstration to the research community of the technology's value. Use this pilot to educate researchers on both the capabilities and the inherent risks of agentic AI.
Phase 2: Build a Secure Foundation (Months 6-18). Before scaling the deployment of agents and tools, it is critical to invest in the foundational infrastructure required for security and governance. This includes establishing a central, Zero Trust gateway to handle authentication, fine-grained authorization (RBAC), and immutable audit logging for all MCP traffic. During this phase, develop a private, internal, trusted registry for hosting and distributing vetted scientific MCP servers. Mandate that all servers be deployed as sandboxed containers and integrate automated security scanning into the development and deployment pipeline.
Phase 3: Curate and Scale (Months 18-36). With a secure foundation in place, the focus can shift to scaling the ecosystem. Prioritize the development of MCP wrappers for the most critical, high-leverage shared research cyberinfrastructure, such as HPC schedulers (Slurm, PBS), data transfer services (Globus), and core institutional databases. Foster an internal open-source culture, creating incentives and providing templates to encourage individual labs and research groups to develop and contribute new scientific tool servers to the private registry.
Phase 4: Govern and Evolve (Ongoing). The agentic ecosystem is not a static, "fire-and-forget" system. Establish a formal governance body—an "Agentic AI Steering Committee"—comprising representatives from research, IT, and security. This body will be responsible for setting security policies, defining the vetting process for new tools in the registry, and managing the overall health of the ecosystem. It must continuously monitor the external security landscape, adapting institutional best practices as the technology, the threat models, and the scientific applications of agentic AI evolve.
The long-term success of MCP in science will ultimately depend more on this commitment to governance, community-building, and trust than on the technical merits of the protocol alone. While the technology provides a solid foundation, the most significant challenges are socio-technical in nature: ensuring quality control, fostering trust in autonomous systems, and developing shared standards.22 The most impactful investment an institution can make is not just in writing code, but in creating the governance structures and community platforms that foster high-quality, secure, and collaborative contributions.
Finally, the rise of agentic science will compel the research community to re-evaluate core scientific principles. When an agent can autonomously chain together dozens of tools across multiple systems to produce a novel result, how is that complex workflow documented to ensure reproducibility? The immutable audit log, initially conceived as a security control, becomes the essential digital lab notebook required for scientific validation. Furthermore, when an agent makes a novel discovery or designs a successful experiment, how is authorship and credit attributed? These are profound questions that go to the heart of the scientific enterprise. The adoption of MCP-enabled agents is not merely an infrastructure upgrade; it is a disruptive event that will necessitate the development of new community norms, standards, and ethical frameworks for documenting, validating, and attributing the discoveries of our future AI research partners.
Works cited
Agentic AI for Scientific Discovery: A Survey of Progress, Challenges, and Future Directions, accessed October 18, 2025, https://arxiv.org/html/2503.08979v1
LLM4SR: A Survey on Large Language Models for Scientific Research - arXiv, accessed October 18, 2025, https://arxiv.org/html/2501.04306v1
Introducing the Model Context Protocol \ Anthropic, accessed October 18, 2025, https://www.anthropic.com/news/model-context-protocol
What Is the Model Context Protocol (MCP) and How It Works - Descope, accessed October 18, 2025, https://www.descope.com/learn/post/mcp
The Model Context Protocol (MCP) in Life Sciences | USDM, accessed October 18, 2025, https://usdm.com/resources/blogs/the-model-context-protocol-mcp-in-life-sciences
Model Context Protocol - Wikipedia, accessed October 18, 2025, https://en.wikipedia.org/wiki/Model_Context_Protocol
What is Model Context Protocol (MCP)? A guide | Google Cloud, accessed October 18, 2025, https://cloud.google.com/discover/what-is-model-context-protocol
What is Model Context Protocol (MCP)? - IBM, accessed October 18, 2025, https://www.ibm.com/think/topics/model-context-protocol
The Model Context Protocol (MCP): A Universal Standard for AI ..., accessed October 18, 2025, https://www.edlitera.com/blog/posts/model-context-protocol
MCP Science: Your AI Co-Pilot for Scientific Discovery, accessed October 18, 2025, https://skywork.ai/skypage/en/mcp-science-ai-co-pilot/1977622351473340416
A Survey of the Model Context Protocol (MCP): Standardizing Context to Enhance Large Language Models (LLMs) - Preprints.org, accessed October 18, 2025, https://www.preprints.org/manuscript/202504.0245/v1
Model Context Protocol (MCP) - Claude Docs, accessed October 18, 2025, https://docs.claude.com/en/docs/mcp
Experiences with Model Context Protocol Servers for Science ... - arXiv, accessed October 18, 2025, https://arxiv.org/abs/2508.18489
Specification - Model Context Protocol, accessed October 18, 2025, https://modelcontextprotocol.io/specification/2025-03-26
MCP Explained: The New Standard Connecting AI to Everything | by Edwin Lisowski, accessed October 18, 2025, https://medium.com/@elisowski/mcp-explained-the-new-standard-connecting-ai-to-everything-79c5a1c98288
Use MCP servers in VS Code, accessed October 18, 2025, https://code.visualstudio.com/docs/copilot/customization/mcp-servers
How is JSON-RPC used in the Model Context Protocol? - Milvus, accessed October 18, 2025, https://milvus.io/ai-quick-reference/how-is-jsonrpc-used-in-the-model-context-protocol
JSON-RPC 2.0 Specification, accessed October 18, 2025, https://www.jsonrpc.org/specification
MCP Message Types: Complete MCP JSON-RPC Reference Guide - Portkey, accessed October 18, 2025, https://portkey.ai/blog/mcp-message-types-complete-json-rpc-reference-guide/
What is the Model Context Protocol (MCP)? A Complete Guide - Treblle, accessed October 18, 2025, https://treblle.com/blog/model-context-protocol-guide
Specification - Model Context Protocol, accessed October 18, 2025, https://modelcontextprotocol.io/specification/2025-06-18
Unveiling Attack Vectors in the Model Context Protocol (MCP ... - arXiv, accessed October 18, 2025, https://arxiv.org/abs/2506.02040
cyanheads/model-context-protocol-resources: Exploring ... - GitHub, accessed October 18, 2025, https://github.com/cyanheads/model-context-protocol-resources
MCP Server Security Best Practices | TrueFoundry, accessed October 18, 2025, https://www.truefoundry.com/blog/mcp-server-security-best-practices
Securing MCP Servers - AWS in Plain English, accessed October 18, 2025, https://aws.plainenglish.io/securing-mcp-servers-4a1872b530cf
Security Best Practices - Model Context Protocol, accessed October 18, 2025, https://modelcontextprotocol.io/specification/draft/basic/security_best_practices
The MCP Security Survival Guide: Best Practices, Pitfalls, and Real-World Lessons, accessed October 18, 2025, https://towardsdatascience.com/the-mcp-security-survival-guide-best-practices-pitfalls-and-real-world-lessons/
Agent Action Schema: The Foundation of Intelligent Automation - Adopt AI, accessed October 18, 2025, https://www.adopt.ai/glossary/agent-action-schema
Model Context Protocol (MCP): Landscape, Security Threats ... - arXiv, accessed October 18, 2025, https://arxiv.org/abs/2503.23278
5 Best Practices for Building MCP Servers | Snyk, accessed October 18, 2025, https://snyk.io/articles/5-best-practices-for-building-mcp-servers/
OpenAPI Specification: A Crucial Tool in the Age of AI | Baresquare, accessed October 18, 2025, https://baresquare.com/blog/openapi-specification
Empowering AI Agents with Tools via OpenAPI | by Akshay Kokane | Medium, accessed October 18, 2025, https://medium.com/@akshaykokane09/empowering-ai-agents-with-tools-via-openapi-specification-a-step-by-step-guide-using-microsofts-16ec3459ab3c
Building an AI agent with OpenAPI: LangChain vs. Haystack - Speakeasy, accessed October 18, 2025, https://www.speakeasy.com/blog/langchain-vs-haystack-api-tools
OpenAPI tools - Agent Development Kit - Google, accessed October 18, 2025, https://google.github.io/adk-docs/tools/openapi-tools/
kubernetes - REST vs gRPC: when should I choose one over the ..., accessed October 18, 2025, https://stackoverflow.com/questions/45625886/rest-vs-grpc-when-should-i-choose-one-over-the-other
ScienceOne-AI/Agent-Interaction-Protocol: Agent Interaction Protocol is a distributed agent interaction protocol. It defines communication mechanisms for multi-agent collaboration, multi-tool invocation, and multi-modal data access in scientific scenarios. - GitHub, accessed October 18, 2025, https://github.com/ScienceOne-AI/Agent-Interaction-Protocol
MCP.science: AI Model Integration for Scientific Research - MCP Market, accessed October 18, 2025, https://mcpmarket.com/server/mcp-science
Path Integral Institute, accessed October 18, 2025, https://www.pathintegral.xyz/
mcp.science · PyPI, accessed October 18, 2025, https://pypi.org/project/mcp.science/
pathintegral-institute/mcp.science: Open Source MCP ... - GitHub, accessed October 18, 2025, https://github.com/pathintegral-institute/mcp.science
Path Integral Institute - GitHub, accessed October 18, 2025, https://github.com/pathintegral-institute
pathintegral-institute/mcpm.sh: CLI MCP package manager & registry for all platforms and all clients. Search & configure MCP servers. Advanced Router & Profile features. - GitHub, accessed October 18, 2025, https://github.com/pathintegral-institute/mcpm.sh
What Are AI Agent Protocols? - IBM, accessed October 18, 2025, https://www.ibm.com/think/topics/ai-agent-protocols
An open-source protocol for AI agents to interact - IBM Research, accessed October 18, 2025, https://research.ibm.com/blog/agent-communication-protocol-ai
[2509.25292] A Measurement Study of Model Context Protocol - arXiv, accessed October 18, 2025, https://www.arxiv.org/abs/2509.25292
Some of the open source standards used with AI agents or agentic frameworks | Fabrix.ai, accessed October 18, 2025, https://fabrix.ai/blog/some-of-the-open-source-standards-used-with-ai-agents-or-agentic-frameworks/
blazickjp/arxiv-mcp-server: A Model Context Protocol server for searching and analyzing arXiv papers - GitHub, accessed October 18, 2025, https://github.com/blazickjp/arxiv-mcp-server
